{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from lxml import etree\n",
    "from glob import glob\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# For scraping\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Conf Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "JRN_CODE = 'afar'\n",
    "LICENCE_FOLDER = 'license_info'\n",
    "TEST_FOLDER = 'test_input'\n",
    "ERROR_FOLDER = 'logs'\n",
    "logging.basicConfig(filename='logs/log_file_{}_{}.log'.format(JRN_CODE, dt_string), filemode='w', format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logging.info('running')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Scraping Atypon Site ####\n",
    "## (Skip this section if you already have the CSV files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"/Users/kmcdouga/Dropbox (MIT)/Silverchair/Batch_2/venv/bin/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loi_url = 'https://www.mitpressjournals.org/loi/{}'.format(JRN_CODE)\n",
    "driver.get(loi_url)\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loi_content = soup.find_all('div', {'class':'js_issue'})\n",
    "urls = []\n",
    "for x in loi_content:\n",
    "    link_tag =  x.find('a', recursive=False)['href']\n",
    "    full_url = 'https://www.mitpressjournals.org{}'.format(link_tag)\n",
    "    urls.append(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.mitpressjournals.org/toc/daed/149/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/149/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/149/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/149/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/148/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/148/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/148/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/148/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/147/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/147/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/147/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/147/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/146/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/146/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/146/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/146/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/145/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/145/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/145/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/145/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/144/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/144/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/144/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/144/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/143/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/143/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/143/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/143/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/142/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/142/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/142/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/142/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/141/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/141/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/141/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/141/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/140/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/140/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/140/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/140/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/139/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/139/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/139/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/139/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/138/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/138/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/138/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/138/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/137/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/137/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/137/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/137/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/136/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/136/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/136/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/136/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/135/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/135/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/135/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/135/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/134/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/134/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/134/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/134/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/133/4',\n",
       " 'https://www.mitpressjournals.org/toc/daed/133/3',\n",
       " 'https://www.mitpressjournals.org/toc/daed/133/2',\n",
       " 'https://www.mitpressjournals.org/toc/daed/133/1',\n",
       " 'https://www.mitpressjournals.org/toc/daed/132/4']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_toc():\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        doi_list, access_list = parse_html(soup, url)\n",
    "    return doi_list, access_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_list = []\n",
    "doi_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = []\n",
    "def parse_html(soup, url):\n",
    "    try:\n",
    "        # We want to grab the tocContent element\n",
    "        toc_content = soup.find(\"div\", {\"class\": \"tocContent\"})\n",
    "        article_entry = toc_content.find_all(\"table\", {\"class\": \"articleEntry\"}, recursive=False)\n",
    "        for article in article_entry:\n",
    "            access_list.append(article.find(\"img\", {\"class\": \"accessIcon\"}, recursive=True)['title'])\n",
    "            doi_list.append(article.find(\"input\", {\"class\": \"tocToolCheckBox\"}, recursive=True)['value'])\n",
    "        return doi_list, access_list\n",
    "    except NoneType as ne:\n",
    "        print(ne)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_list, access_list = scrape_toc()\n",
    "zipped = list(zip(doi_list, access_list))\n",
    "df = pd.DataFrame(zipped,  columns=['DOI', 'ACCESS'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've scraped all the file info\n",
    "# We generate  a DF that holds all the access data and adds a new column for normalized article IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doi(row):\n",
    "    full_doi = row['DOI']\n",
    "    normalized_doi = full_doi.split('/')[1].upper()\n",
    "    return normalized_doi\n",
    "\n",
    "df['NORMLIZED_DOI'] = df.apply(normalize_doi, axis=1)\n",
    "\n",
    "# Save everything to a file, just in case\n",
    "df.to_csv('{}/access_list_{}.csv'.format(LICENCE_FOLDER, JRN_CODE), index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Checking and Updating the XML files ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded:                        DOI     ACCESS NORMLIZED_DOI\n",
      "0     10.1162/afar_a_00545  No Access  AFAR_A_00545\n",
      "1     10.1162/afar_a_00546  No Access  AFAR_A_00546\n",
      "2     10.1162/afar_a_00547  No Access  AFAR_A_00547\n",
      "3     10.1162/afar_a_00548  No Access  AFAR_A_00548\n",
      "4     10.1162/afar_a_00549  No Access  AFAR_A_00549\n",
      "...                    ...        ...           ...\n",
      "4863       10.2307/3334364  No Access       3334364\n",
      "4864       10.2307/3334365  No Access       3334365\n",
      "4865       10.2307/3334366  No Access       3334366\n",
      "4866       10.2307/3334367  No Access       3334367\n",
      "4867       10.2307/3334368  No Access       3334368\n",
      "\n",
      "[4868 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "license_df = pd.read_csv('{}/access_list_{}.csv'.format(LICENCE_FOLDER, JRN_CODE))\n",
    "print('loaded: {}'.format(license_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the license_df to only include OA articles\n",
    "# Filter the license_df to only include Free article\n",
    "# The gated articles don't require any changes.\n",
    "oa_articles = license_df[license_df['ACCESS']=='Open Access']\n",
    "free_articles = license_df[license_df['ACCESS']=='Free Access']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [DOI, ACCESS, NORMLIZED_DOI]\n",
      "Index: []\n",
      "                           DOI       ACCESS      NORMLIZED_DOI\n",
      "7         10.1162/afar_a_00552  Free Access       AFAR_A_00552\n",
      "17        10.1162/afar_a_00536  Free Access       AFAR_A_00536\n",
      "28        10.1162/afar_a_00525  Free Access       AFAR_A_00525\n",
      "37        10.1162/afar_a_00511  Free Access       AFAR_A_00511\n",
      "49        10.1162/afar_a_00495  Free Access       AFAR_A_00495\n",
      "55        10.1162/afar_a_00501  Free Access       AFAR_A_00501\n",
      "63        10.1162/afar_a_00473  Free Access       AFAR_A_00473\n",
      "64        10.1162/afar_a_00474  Free Access       AFAR_A_00474\n",
      "67        10.1162/afar_a_00478  Free Access       AFAR_A_00478\n",
      "89        10.1162/afar_a_00456  Free Access       AFAR_A_00456\n",
      "90        10.1162/afar_a_00457  Free Access       AFAR_A_00457\n",
      "105       10.1162/afar_a_00440  Free Access       AFAR_A_00440\n",
      "111       10.1162/afar_a_00446  Free Access       AFAR_A_00446\n",
      "120       10.1162/afar_x_00428  Free Access       AFAR_X_00428\n",
      "122       10.1162/afar_a_00430  Free Access       AFAR_A_00430\n",
      "135       10.1162/afar_a_00414  Free Access       AFAR_A_00414\n",
      "148       10.1162/afar_a_00399  Free Access       AFAR_A_00399\n",
      "152       10.1162/afar_a_00403  Free Access       AFAR_A_00403\n",
      "163       10.1162/AFAR_a_00390  Free Access       AFAR_A_00390\n",
      "175       10.1162/AFAR_a_00372  Free Access       AFAR_A_00372\n",
      "191       10.1162/AFAR_a_00353  Free Access       AFAR_A_00353\n",
      "206       10.1162/AFAR_a_00340  Free Access       AFAR_A_00340\n",
      "207       10.1162/AFAR_a_00341  Free Access       AFAR_A_00341\n",
      "224       10.1162/AFAR_a_00330  Free Access       AFAR_A_00330\n",
      "226       10.1162/AFAR_a_00332  Free Access       AFAR_A_00332\n",
      "240       10.1162/AFAR_a_00314  Free Access       AFAR_A_00314\n",
      "251       10.1162/AFAR_a_00296  Free Access       AFAR_A_00296\n",
      "268       10.1162/AFAR_a_00286  Free Access       AFAR_A_00286\n",
      "279       10.1162/AFAR_a_00267  Free Access       AFAR_A_00267\n",
      "293       10.1162/AFAR_a_00250  Free Access       AFAR_A_00250\n",
      "311       10.1162/AFAR_a_00239  Free Access       AFAR_A_00239\n",
      "325       10.1162/AFAR_a_00217  Free Access       AFAR_A_00217\n",
      "343       10.1162/AFAR_a_00197  Free Access       AFAR_A_00197\n",
      "360       10.1162/AFAR_a_00182  Free Access       AFAR_A_00182\n",
      "373       10.1162/AFAR_a_00163  Free Access       AFAR_A_00163\n",
      "387       10.1162/AFAR_a_00135  Free Access       AFAR_A_00135\n",
      "403       10.1162/AFAR_a_00119  Free Access       AFAR_A_00119\n",
      "421       10.1162/AFAR_a_00104  Free Access       AFAR_A_00104\n",
      "443       10.1162/AFAR_a_00089  Free Access       AFAR_A_00089\n",
      "462       10.1162/AFAR_a_00067  Free Access       AFAR_A_00067\n",
      "477       10.1162/AFAR_a_00044  Free Access       AFAR_A_00044\n",
      "493       10.1162/AFAR_a_00028  Free Access       AFAR_A_00028\n",
      "505       10.1162/AFAR_a_00009  Free Access       AFAR_A_00009\n",
      "518  10.1162/afar.2012.45.2.12  Free Access  AFAR.2012.45.2.12\n",
      "536  10.1162/afar.2012.45.1.16  Free Access  AFAR.2012.45.1.16\n",
      "554  10.1162/afar.2011.44.4.52  Free Access  AFAR.2011.44.4.52\n",
      "555  10.1162/afar.2011.44.4.64  Free Access  AFAR.2011.44.4.64\n",
      "571  10.1162/afar.2011.44.3.16  Free Access  AFAR.2011.44.3.16\n",
      "584   10.1162/afar.2011.44.2.8  Free Access   AFAR.2011.44.2.8\n",
      "601  10.1162/afar.2011.44.1.26  Free Access  AFAR.2011.44.1.26\n",
      "679  10.1162/afar.2009.42.4.38  Free Access  AFAR.2009.42.4.38\n",
      "799  10.1162/afar.2007.40.4.44  Free Access  AFAR.2007.40.4.44\n",
      "809  10.1162/afar.2007.40.3.12  Free Access  AFAR.2007.40.3.12\n",
      "820  10.1162/afar.2007.40.2.16  Free Access  AFAR.2007.40.2.16\n",
      "831  10.1162/afar.2007.40.1.10  Free Access  AFAR.2007.40.1.10\n"
     ]
    }
   ],
   "source": [
    "print(oa_articles)\n",
    "print(free_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order for lxml to process the files we need to provide the below info\n",
    "nsmap = {\n",
    "'xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
    "'xlink': 'http://www.w3.org/1999/xlink',\n",
    "}\n",
    "attr_qname = etree.QName(\"http://www.w3.org/2001/XMLSchema-instance\", \"schemaLocation\")\n",
    "\n",
    "parser = etree.XMLParser(recover=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we look for the issue_meta file and update the TOC as needed\n",
    "# Next we look for OA and Free articles and add the license info as needed\n",
    "\n",
    "def update_files():\n",
    "    for root, directory, files in os.walk(TEST_FOLDER):\n",
    "        for f in files:\n",
    "            # If we have the issue_meta file we need to update it.\n",
    "            # First we grab the path to the issue_meta file.\n",
    "            # We then get df_filter, which holds all the DOIs in this issue\n",
    "            # Then we get unique_headings, which holds the TOC headins if any exist. \n",
    "            # That info has been scraped from the website directly and lives in a CSV file\n",
    "            # that we get from get_doi_list(). \n",
    "            # Once we have that we call convert_issue_xml(), which does the actual updating of the issue files\n",
    "            if 'issue_meta' in f:\n",
    "                path_to_issue_file = '{}/{}'.format(root, f)\n",
    "#                 path_to_assets_folder = '{}/{}'.format(root, )\n",
    "                issue_folder_name = root.split('/')[1]\n",
    "                df_filter, unique_headings = get_doi_list('{}/{}/XML'.format(TEST_FOLDER, issue_folder_name))\n",
    "                converted = convert_issue_xml(path_to_issue_file, df_filter, unique_headings, issue_folder_name)\n",
    "#                 with open('{}'.format(path_to_issue_file), 'wb') as doc:\n",
    "#                     doc.write(etree.tostring(converted, pretty_print = True))\n",
    "            \n",
    "            # If we have an article file we check to see if the license needs to added\n",
    "            # Since the articles are named according to their DOI we can iterate over all the articles \n",
    "            # in each package and updated those in the filtered lists\n",
    "            \n",
    "            # This part is a bit inefficent and could do with some cleanup so that\n",
    "            # we don't have two functions doing more or less the same thing. \n",
    "            # For now, however, it works. \n",
    "            if f.endswith('xml') and 'issue' not in f:\n",
    "                article_path = '{}/{}'.format(root, f)\n",
    "#                 add_review_title(article_path)\n",
    "#                 add_jstor_link(article_path)\n",
    "                article_name = f.split('.xml')[0].upper()\n",
    "#                 if article_name.upper() in oa_articles['NORMLIZED_DOI'].values:\n",
    "#                     update_license_oa(article_path)\n",
    "#                     print(article_path)\n",
    "#                 if article_name.upper() in free_articles['NORMLIZED_DOI'].values:\n",
    "#                     update_license_free(article_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_license_oa(article_path):\n",
    "    tree = etree.parse(article_path, parser=parser)\n",
    "    article_root  = tree.getroot()\n",
    "    # Look for an existing license element\n",
    "    license_tag = tree.find('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}license')\n",
    "    # If we find a license tag in the article we want to make sure it has the right license. \n",
    "    # If it hase a license element it SHOULD have the correct license, but we should check anyway\n",
    "    # If it has the wrong license (anything other than 'open-access') just log it for review later\n",
    "    if license_tag is not None:             \n",
    "        if license_tag.get('license-type') != 'open-access':\n",
    "            logging.info('File: {}. license found with incorrect license-type. Expectng \\'open-access\\' found \\'{}\\''.format(article_path, license_tag.get('license-type')))\n",
    "        if license_tag.get('license-type') == 'open-access':\n",
    "            logging.info('File: {}. license found with correct license-type.'.format(article_path))\n",
    "\n",
    "    # We now have an OA ile without a license in the XML.\n",
    "    # So that means we have to add it\n",
    "    # We do that by finding the permission tag and appending a child\n",
    "    else:\n",
    "        permissions_tag = tree.find('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}permissions')\n",
    "        new_license_tag = etree.SubElement(permissions_tag, 'license')\n",
    "        new_license_tag.attrib['license-type'] = 'open-access'\n",
    "        new_license_tag.attrib['{%s}href' % nsmap['xlink']] = 'https://creativecommons.org/licenses/by/4.0/'\n",
    "\n",
    "        new_license_p = etree.SubElement(new_license_tag, 'license-p')\n",
    "        new_license_p.text = 'This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. For a full description of the license, please visit <ext-link ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/legalcode\">https://creativecommons.org/licenses/by/4.0/legalcode</ext-link>.'\n",
    "\n",
    "        logging.info('File: {}. Updating file with OA license.'.format(article_path))\n",
    "\n",
    "        with open(article_path, 'wb') as doc:\n",
    "            doc.write(etree.tostring(tree, pretty_print = True))\n",
    "        with open(article_path, 'r+') as f:\n",
    "            text = f.read()\n",
    "            text = re.sub('&lt;ext-link ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/legalcode\"&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/ext-link&gt;', '<ext-link ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/legalcode\">https://creativecommons.org/licenses/by/4.0/legalcode</ext-link>', text)\n",
    "            f.seek(0)\n",
    "            f.write(text)\n",
    "            f.truncate()\n",
    "\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_license_free(article_path):\n",
    "    tree = etree.parse(article_path, parser=parser)\n",
    "    article_root  = tree.getroot()\n",
    "\n",
    "    license_tag = tree.find('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}license')\n",
    "    if license_tag is not None:             \n",
    "        logging.info('File: {}. No update to free article. license found with some license. Check before updatingcorrect license-type.'.format(article_path))\n",
    "    else:\n",
    "        permissions_tag = tree.find('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}permissions')\n",
    "        new_license_tag = etree.SubElement(permissions_tag, 'license')\n",
    "        new_license_tag.attrib['license-type'] = 'free'                        \n",
    "\n",
    "        logging.info('File: {}. Updating file with free license.'.format(article_path))\n",
    "\n",
    "        with open(article_path, 'wb') as doc:\n",
    "            doc.write(etree.tostring(tree, pretty_print = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_issue_xml(path_to_issue_file, df_filter, unique_headings, issue_folder_name):\n",
    "\n",
    "    folder = os.path.dirname(os.path.dirname(path_to_issue_file)).split('/')[1]\n",
    "    assets_folder = '{}/{}'.format(os.path.dirname(os.path.dirname(path_to_issue_file)), 'assets')\n",
    "    issue_num = folder.split('-')[1]\n",
    "    \n",
    "    tree = etree.parse(path_to_issue_file, parser=parser)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Some issues are missing their covers and we need to download them from the site\n",
    "    # This checks if a cover exists and if it doesn't it grabs it\n",
    "    def get_cover():\n",
    "        if glob('{}/*cover.*'.format(assets_folder)):\n",
    "            print('cover found in {}'.format(path_to_issue_file))\n",
    "            pass\n",
    "        else:\n",
    "            print('cover missing')\n",
    "            # Check if webdriver running\n",
    "            try:\n",
    "                driver.title\n",
    "                print(True)\n",
    "            except NameError as e:\n",
    "                print(False)\n",
    "                driver = webdriver.Chrome(\"/Users/kmcdouga/Dropbox (MIT)/Silverchair/Batch_2/venv/bin/chromedriver\")\n",
    "\n",
    "            # The URL to the issue homepage follows a pattern that we can stitch together using the volume and issue\n",
    "            volume = root.find('.//{http://specifications.silverchair.com/xsd/1/21/SCJATS-journalissue.xsd}volume').text\n",
    "            issue = root.find('.//{http://specifications.silverchair.com/xsd/1/21/SCJATS-journalissue.xsd}issue').text\n",
    "            toc_path = 'https://www.mitpressjournals.org/toc/{}/{}/{}'.format(JRN_CODE, volume, issue)\n",
    "            # toc_path = 'https://www.mitpressjournals.org/toc/{}/{}'.format(JRN_CODE, issue)\n",
    "#             toc_path = 'https://www.mitpressjournals.org/toc/{}/{}'.format(JRN_CODE, volume)\n",
    "\n",
    "            try:\n",
    "                driver.get(toc_path)\n",
    "                content = driver.page_source\n",
    "                soup = BeautifulSoup(content, features='lxml')\n",
    "            \n",
    "                cover_tag = soup.find_all('img', {'alt':'Publication Cover'})\n",
    "                cover_path = 'https://www.mitpressjournals.org/{}'.format(cover_tag[0]['src'])\n",
    "                cover_name = cover_path.split('/')[-1]\n",
    "            \n",
    "                supplementary_material = etree.SubElement(root, 'supplementary-material')\n",
    "                supplementary_material.attrib['{%s}href' % nsmap['xlink']] = cover_name\n",
    "                supplementary_material.attrib['content-type'] = 'cover'\n",
    "            \n",
    "                response = requests.get(cover_path)\n",
    "                save_path = '{}/{}'.format(assets_folder, cover_name)\n",
    "                file = open(save_path, \"wb\")\n",
    "                file.write(response.content)\n",
    "                file.close()\n",
    "                driver.quit()\n",
    "            except Exception as e:\n",
    "                print('error getting cover: {}'.format(e))\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    get_cover()\n",
    "    # issue_identifier = root.find('.//{http://specifications.silverchair.com/xsd/1/21/SCJATS-journalissue.xsd}issue-identifier')\n",
    "    # vol = etree.SubElement(issue_identifier, 'volume')\n",
    "    # vol.attrib['content-type'] = 'empty'\n",
    "\n",
    "    # issue = etree.SubElement(issue_identifier, 'issue')\n",
    "    # issue.text = issue_num\n",
    "\n",
    "    \n",
    "    \n",
    "    # Now we update the issue_meta file adding the <toc> block\n",
    "    toc_block = root.find('.//{http://specifications.silverchair.com/xsd/1/21/SCJATS-journalissue.xsd}table-of-contents')\n",
    "    toc_block.getparent().remove(toc_block)\n",
    "\n",
    "\n",
    "    table_of_contents = etree.SubElement(root, 'table-of-contents')\n",
    "    \n",
    "    # Look for articles without a heading in the TOC\n",
    "    no_heading = df_filter[pd.isna(df_filter['article_title'])]\n",
    "\n",
    "    \n",
    "#     toc_sec = etree.SubElement(table_of_contents, 'toc-sec')\n",
    "#     toc_title = etree.SubElement(toc_sec, 'title')\n",
    "\n",
    "    if not no_heading.empty:\n",
    "        toc_sec = etree.SubElement(table_of_contents, 'toc-sec')\n",
    "        toc_title = etree.SubElement(toc_sec, 'title')\n",
    "        for index, row in no_heading.iterrows():\n",
    "            toc_content = etree.SubElement(toc_sec, 'toc-content')\n",
    "            toc_content.attrib['pub-id-type'] = 'doi'\n",
    "            toc_content.attrib['{%s}href' % nsmap['xlink']] = row['article_doi']\n",
    "\n",
    "    for heading in unique_headings:\n",
    "        toc_sec = etree.SubElement(table_of_contents, 'toc-sec')\n",
    "        toc_title = etree.SubElement(toc_sec, 'title')\n",
    "        toc_title.text = heading\n",
    "        df_sec = df_filter[df_filter.article_title == heading]\n",
    "        for index, row in df_sec.iterrows():\n",
    "            toc_content = etree.SubElement(toc_sec, 'toc-content')\n",
    "            toc_content.attrib['pub-id-type'] = 'doi'\n",
    "            toc_content.attrib['{%s}href' % nsmap['xlink']] = row['article_doi']\n",
    "    return root\n",
    "\n",
    "def get_doi_list(path):\n",
    "    delete_command = 'find \\\\{} -name \\'*.DS_Store*\\' -delete'.format(path)\n",
    "    os.system(delete_command)\n",
    "\n",
    "    doi_list = ['10.1162/{}'.format(os.path.splitext(item)[0].upper()) for item in os.listdir(path)]\n",
    "#     doi_list = ['10.2307/{}'.format(os.path.splitext(item)[0].upper()) for item in os.listdir(path)]\n",
    "\n",
    "    \n",
    "    df = pd.read_csv('{}.csv'.format(JRN_CODE))\n",
    "\n",
    "    def popTime(row):\n",
    "        x = row['article_doi']\n",
    "        return(x.upper())\n",
    "\n",
    "    df['article_doi'] = df.apply(popTime, axis=1)\n",
    "    df_filter = df[df.article_doi.isin(doi_list)]\n",
    "\n",
    "    unique_headings = pd.unique(df_filter['article_title'])\n",
    "    unique_headings = [value for value in unique_headings if pd.notna(value)]\n",
    "\n",
    "    return df_filter, unique_headings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jstor_link(article_path):\n",
    "    print(article_path)\n",
    "    tree = etree.parse(article_path)\n",
    "    article_root  = tree.getroot()\n",
    "\n",
    "    article_id_tags = tree.findall('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}article-id')\n",
    "\n",
    "    for tag in article_id_tags:\n",
    "        if tag.get(\"pub-id-type\") == 'doi':\n",
    "            permissions_tag = tree.find('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}permissions')\n",
    "\n",
    "            abs_tag = etree.Element(\"abstract\")\n",
    "            permissions_tag.addnext(abs_tag)\n",
    "\n",
    "            p_tag = etree.SubElement(abs_tag, 'p')\n",
    "            p_tag.text = 'Available on JSTOR at: doi.org/{}'.format(tag.text)\n",
    "\n",
    "        if tag.get(\"pub-id-type\") == 'jstor':\n",
    "            comment = etree.Comment(' === <article-id pub-id-type=\"jstor\">{}</article-id> === '.format(tag.text))\n",
    "            article_id_tags[0].addnext(comment)\n",
    "\n",
    "            tag.getparent().remove(tag)\n",
    "            \n",
    "    with open(article_path, 'wb') as doc:\n",
    "        doc.write(etree.tostring(tree, pretty_print = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_review_title(article_path):\n",
    "    tree = etree.parse(article_path, parser=parser)\n",
    "    article_root  = tree.getroot()\n",
    "\n",
    "    if article_root.get('article-type') == 'other' or article_root.get('article-type') == 'book-review' or article_root.get('article-type') == 'review-article':\n",
    "\n",
    "        try:\n",
    "            # See if there is an article title\n",
    "            article_title_tag = tree.xpath('///xsi:article/xsi:front/xsi:article-meta/xsi:title-group/xsi:article-title', namespaces={'xsi':'http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd'})\n",
    "            # Check if there is a product > source tag\n",
    "            product_source_tags = tree.findall('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}source')\n",
    "            source_tags_text = [ tag.text for tag in product_source_tags ]\n",
    "            # Check if there is a product > article-title tag\n",
    "            review_title_tags = tree.xpath('///xsi:article/xsi:front/xsi:article-meta/xsi:product/xsi:article-title', namespaces={'xsi':'http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd'})\n",
    "            review_title_tags_text = [ tag.text for tag in review_title_tags ]\n",
    "            # Check if there is a product > article-title >italic tag\n",
    "            review_title_italic_tags = tree.xpath('///xsi:article/xsi:front/xsi:article-meta/xsi:product/xsi:article-title/xsi:italic', namespaces={'xsi':'http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd'})\n",
    "            review_title_italic_tags_text = [ tag.text for tag in review_title_italic_tags ]\n",
    "\n",
    "            # If the article-title tag is empty\n",
    "            if article_title_tag[0].text == None:\n",
    "                if len(review_title_italic_tags) != 0:\n",
    "                    title_text = \"; \".join(review_title_italic_tags)\n",
    "                    article_title_tag[0].text = title_text\n",
    "                    print(article_path)\n",
    "                    print(title_text)\n",
    "                elif len(review_title_tags_text) != 0:\n",
    "                    title_text = \"; \".join(review_title_tags_text)\n",
    "                    article_title_tag[0].text = title_text\n",
    "                    print(article_path)\n",
    "                    print(title_text)\n",
    "                elif len(source_tags_text) != 0:\n",
    "                    title_text = \"; \".join(source_tags_text)\n",
    "                    article_title_tag[0].text = title_text\n",
    "                    print(article_path)\n",
    "                    print(title_text)\n",
    "                else:\n",
    "                    print('no product element in: {}'.format(article_path))\n",
    "        except Exception as e:\n",
    "            print('{} in {}'.format(e, article_path))\n",
    "\n",
    "#     with open(article_path, 'wb') as doc:\n",
    "#         doc.write(etree.tostring(tree, pretty_print = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover found in test_input/afar-40-1/xml/issue_meta.xml\n",
      "cover found in test_input/afar-40-2/xml/issue_meta.xml\n",
      "cover found in test_input/afar-40-3/xml/issue_meta.xml\n",
      "cover found in test_input/afar-40-4/xml/issue_meta.xml\n",
      "cover found in test_input/afar-41-1/xml/issue_meta.xml\n",
      "cover found in test_input/afar-41-2/xml/issue_meta.xml\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "error getting cover: list index out of range\n",
      "cover missing\n",
      "False\n",
      "error getting cover: list index out of range\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n",
      "cover missing\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "update_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for Checking and Updating the XML files ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Caused by line breaks in source XML. Need to find *anything* after surname and string-name and remove accordingly. \n",
    "- all files need to be formatted and indented. \n",
    "\n",
    "Need to join and normalize to start:\n",
    "F: \\s+\n",
    "R: ''\n",
    "\n",
    "From there we need to fix formatting:\n",
    "F: </surname> </\n",
    "R: </surname></\n",
    "\n",
    "F: </string-name> </\n",
    "R: </string-name></\n",
    "\n",
    "F: </given-names> </string-name>\n",
    "R: </given-names></string-name>\n",
    "\n",
    "F: </collab> </person-group>\n",
    "R: </collab></person-group> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caused by line breaks in source XML. Need to find *anything* after surname and string-name and remove accordingly. \n",
    "# - all files need to be formatted and indented. \n",
    "\n",
    "# Need to join and normalize to start:\n",
    "# F: \\s+\n",
    "# R: ''\n",
    "\n",
    "# From there we need to fix formatting:\n",
    "# F: </surname> </\n",
    "# R: </surname></\n",
    "\n",
    "# F: </string-name> </\n",
    "# R: </string-name></\n",
    "\n",
    "# F: </given-names> </string-name>\n",
    "# R: </given-names></string-name>\n",
    "\n",
    "# F: </collab> </person-group>\n",
    "# R: </collab></person-group>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "import glob, os\n",
    "import logging\n",
    "\n",
    "\n",
    "# logging.basicConfig(filename='log_file.log', filemode='w', format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "def get_eps_files():\n",
    "    # df_filter, unique_headings = get_doi_list('{}/{}'.format(input_path, issue_folder))\n",
    "\n",
    "    delete_command = 'find \\\\output -name \\'*.DS_Store*\\' -delete'\n",
    "    os.system(delete_command)\n",
    "    for root, directory, files in os.walk(TEST_FOLDER):\n",
    "\n",
    "        # Find the assets folder\n",
    "        for d in directory:\n",
    "            if d == 'assets':\n",
    "                # Find the .eps.gz files\n",
    "                assets_path = '{}/{}'.format(root, d)\n",
    "                gz_files = get_gz_files(assets_path)\n",
    "                print(gz_files)\n",
    "#                 unzip_gz_files(gz_files)\n",
    "\n",
    "def get_gz_files(assets_path):\n",
    "    gz_files = [] # List to hold all the .eps.gz files\n",
    "    for file in os.listdir(assets_path):\n",
    "        if file.endswith(\".eps.gz\"):\n",
    "            gz_files.append('{}/{}'.format(assets_path, file))\n",
    "    return gz_files\n",
    "\n",
    "def unzip_gz_files(gz_files):\n",
    "    for file in gz_files:\n",
    "        unzip_command = 'gzip -d {}'.format(file) # Will unzip and automatically delete .gz files\n",
    "        os.system(unzip_command)\n",
    "        print(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_eps_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import shutil\n",
    "# import glob, os\n",
    "# import logging\n",
    "\n",
    "\n",
    "# # logging.basicConfig(filename='log_file.log', filemode='w', format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def run():\n",
    "#     delete_command = 'find \\\\output -name \\'*.DS_Store*\\' -delete'\n",
    "#     os.system(delete_command)\n",
    "#     for root, directory, files in os.walk(output_path):\n",
    "\n",
    "#         # Find the assets folder\n",
    "#         for d in directory:\n",
    "#             if d == 'assets':\n",
    "#                 # Find the .eps.gz files\n",
    "#                 assets_path = '{}/{}'.format(root, d)\n",
    "#                 gz_files = get_gz_files(assets_path)\n",
    "#                 unzip_gz_files(gz_files)\n",
    "\n",
    "\n",
    "# def get_eps_files(assets_path):\n",
    "#     gz_files = [] # List to hold all the .eps.gz files\n",
    "#     for file in os.listdir(assets_path):\n",
    "#         if file.endswith(\".eps\"):\n",
    "#             gz_files.append('{}/{}'.format(assets_path, file))\n",
    "#     return gz_files\n",
    "\n",
    "# def delete_eps_files(gz_files):\n",
    "#     for file in gz_files:\n",
    "#         delete_command = 'rm {}'.format(file) # Will unzip and automatically delete .gz files\n",
    "#         os.system(delete_command)\n",
    "#         print(file)\n",
    "\n",
    "# def zip_issues(folder_names):\n",
    "#     for folder_name in folder_names:\n",
    "#         zip_command = 'zip -r output/{}.zip output/{}'.format(folder_name, folder_name)\n",
    "#         print(zip_command)\n",
    "#         os.system(zip_command)\n",
    "\n",
    "# run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
