{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from lxml import etree\n",
    "from glob import glob\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# For scraping\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Conf Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "JRN_CODE = 'afar'\n",
    "LICENCE_FOLDER = 'license_info'\n",
    "TEST_FOLDER = 'test_input'\n",
    "ERROR_FOLDER = 'logs'\n",
    "WD_PATH = 'venv/bin/chromedriver'\n",
    "logging.basicConfig(filename='logs/log_file_{}_{}.log'.format(JRN_CODE, dt_string), filemode='w', format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logging.info('running')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Scraping  Site ####\n",
    "## (Skip this section if you already have the CSV files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = webdriver.Chrome(WD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loi_url = 'https://www.mitpressjournals.org/loi/{}'.format(JRN_CODE)\n",
    "driver.get(loi_url)\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loi_content = soup.find_all('div', {'class':'js_issue'})\n",
    "urls = []\n",
    "for x in loi_content:\n",
    "    link_tag =  x.find('a', recursive=False)['href']\n",
    "    full_url = 'https://www.mitpressjournals.org{}'.format(link_tag)\n",
    "    urls.append(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_toc():\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content)\n",
    "        doi_list, access_list = parse_html(soup, url)\n",
    "    return doi_list, access_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_list = []\n",
    "doi_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = []\n",
    "def parse_html(soup, url):\n",
    "    try:\n",
    "        # We want to grab the tocContent element\n",
    "        toc_content = soup.find(\"div\", {\"class\": \"tocContent\"})\n",
    "        article_entry = toc_content.find_all(\"table\", {\"class\": \"articleEntry\"}, recursive=False)\n",
    "        for article in article_entry:\n",
    "            access_list.append(article.find(\"img\", {\"class\": \"accessIcon\"}, recursive=True)['title'])\n",
    "            doi_list.append(article.find(\"input\", {\"class\": \"tocToolCheckBox\"}, recursive=True)['value'])\n",
    "        return doi_list, access_list\n",
    "    except NoneType as ne:\n",
    "        print(ne)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_list, access_list = scrape_toc()\n",
    "zipped = list(zip(doi_list, access_list))\n",
    "df = pd.DataFrame(zipped,  columns=['DOI', 'ACCESS'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've scraped all the file info\n",
    "# We generate  a DF that holds all the access data and adds a new column for normalized article IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doi(row):\n",
    "    full_doi = row['DOI']\n",
    "    normalized_doi = full_doi.split('/')[1].upper()\n",
    "    return normalized_doi\n",
    "\n",
    "df['NORMLIZED_DOI'] = df.apply(normalize_doi, axis=1)\n",
    "\n",
    "# Save everything to a file, just in case\n",
    "df.to_csv('{}/access_list_{}.csv'.format(LICENCE_FOLDER, JRN_CODE), index=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Checking and Updating the XML files ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "license_df = pd.read_csv('{}/access_list_{}.csv'.format(LICENCE_FOLDER, JRN_CODE))\n",
    "print('loaded: {}'.format(license_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the license_df to only include OA articles\n",
    "# Filter the license_df to only include Free article\n",
    "# The gated articles don't require any changes.\n",
    "oa_articles = license_df[license_df['ACCESS']=='Open Access']\n",
    "free_articles = license_df[license_df['ACCESS']=='Free Access']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oa_articles)\n",
    "print(free_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order for lxml to process the files we need to provide the below info\n",
    "nsmap = {\n",
    "'xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n",
    "'xlink': 'http://www.w3.org/1999/xlink',\n",
    "}\n",
    "attr_qname = etree.QName(\"http://www.w3.org/2001/XMLSchema-instance\", \"schemaLocation\")\n",
    "\n",
    "parser = etree.XMLParser(recover=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we look for the issue_meta file and update the TOC as needed\n",
    "# Next we look for OA and Free articles and add the license info as needed\n",
    "\n",
    "def update_files():\n",
    "    for root, directory, files in os.walk(TEST_FOLDER):\n",
    "        for f in files:\n",
    "            # If we have the issue_meta file we need to update it.\n",
    "            # First we grab the path to the issue_meta file.\n",
    "            # We then get df_filter, which holds all the DOIs in this issue\n",
    "            # Then we get unique_headings, which holds the TOC headins if any exist. \n",
    "            # That info has been scraped from the website directly and lives in a CSV file\n",
    "            # that we get from get_doi_list(). \n",
    "            # Once we have that we call convert_issue_xml(), which does the actual updating of the issue files\n",
    "            if 'issue_meta' in f:\n",
    "                path_to_issue_file = '{}/{}'.format(root, f)\n",
    "#                 path_to_assets_folder = '{}/{}'.format(root, )\n",
    "                issue_folder_name = root.split('/')[1]\n",
    "                df_filter, unique_headings = get_doi_list('{}/{}/XML'.format(TEST_FOLDER, issue_folder_name))\n",
    "                converted = convert_issue_xml(path_to_issue_file, df_filter, unique_headings, issue_folder_name)\n",
    "#                 with open('{}'.format(path_to_issue_file), 'wb') as doc:\n",
    "#                     doc.write(etree.tostring(converted, pretty_print = True))\n",
    "            \n",
    "            # If we have an article file we check to see if the license needs to added\n",
    "            # Since the articles are named according to their DOI we can iterate over all the articles \n",
    "            # in each package and updated those in the filtered lists\n",
    "            \n",
    "            # This part is a bit inefficent and could do with some cleanup so that\n",
    "            # we don't have two functions doing more or less the same thing. \n",
    "            # For now, however, it works. \n",
    "            if f.endswith('xml') and 'issue' not in f:\n",
    "                article_path = '{}/{}'.format(root, f)\n",
    "                article_name = f.split('.xml')[0].upper()\n",
    "                if article_name.upper() in oa_articles['NORMLIZED_DOI'].values:\n",
    "                    update_license_oa(article_path)\n",
    "                    print(article_path)\n",
    "                if article_name.upper() in free_articles['NORMLIZED_DOI'].values:\n",
    "                    update_license_free(article_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_license_oa(article_path):\n",
    "    tree = etree.parse(article_path, parser=parser)\n",
    "    article_root  = tree.getroot()\n",
    "    # Look for an existing license element\n",
    "    license_tag = tree.find('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}license')\n",
    "    # If we find a license tag in the article we want to make sure it has the right license. \n",
    "    # If it hase a license element it SHOULD have the correct license, but we should check anyway\n",
    "    # If it has the wrong license (anything other than 'open-access') just log it for review later\n",
    "    if license_tag is not None:             \n",
    "        if license_tag.get('license-type') != 'open-access':\n",
    "            logging.info('File: {}. license found with incorrect license-type. Expectng \\'open-access\\' found \\'{}\\''.format(article_path, license_tag.get('license-type')))\n",
    "        if license_tag.get('license-type') == 'open-access':\n",
    "            logging.info('File: {}. license found with correct license-type.'.format(article_path))\n",
    "\n",
    "    # We now have an OA ile without a license in the XML.\n",
    "    # So that means we have to add it\n",
    "    # We do that by finding the permission tag and appending a child\n",
    "    else:\n",
    "        permissions_tag = tree.find('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}permissions')\n",
    "        new_license_tag = etree.SubElement(permissions_tag, 'license')\n",
    "        new_license_tag.attrib['license-type'] = 'open-access'\n",
    "        new_license_tag.attrib['{%s}href' % nsmap['xlink']] = 'https://creativecommons.org/licenses/by/4.0/'\n",
    "\n",
    "        new_license_p = etree.SubElement(new_license_tag, 'license-p')\n",
    "        new_license_p.text = 'This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. For a full description of the license, please visit <ext-link ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/legalcode\">https://creativecommons.org/licenses/by/4.0/legalcode</ext-link>.'\n",
    "\n",
    "        logging.info('File: {}. Updating file with OA license.'.format(article_path))\n",
    "\n",
    "        with open(article_path, 'wb') as doc:\n",
    "            doc.write(etree.tostring(tree, pretty_print = True))\n",
    "        with open(article_path, 'r+') as f:\n",
    "            text = f.read()\n",
    "            text = re.sub('&lt;ext-link ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/legalcode\"&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/ext-link&gt;', '<ext-link ext-link-type=\"uri\" xlink:href=\"https://creativecommons.org/licenses/by/4.0/legalcode\">https://creativecommons.org/licenses/by/4.0/legalcode</ext-link>', text)\n",
    "            f.seek(0)\n",
    "            f.write(text)\n",
    "            f.truncate()\n",
    "\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_license_free(article_path):\n",
    "    tree = etree.parse(article_path, parser=parser)\n",
    "    article_root  = tree.getroot()\n",
    "\n",
    "    license_tag = tree.find('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}license')\n",
    "    if license_tag is not None:             \n",
    "        logging.info('File: {}. No update to free article. license found with some license. Check before updatingcorrect license-type.'.format(article_path))\n",
    "    else:\n",
    "        permissions_tag = tree.find('.//{http://specifications.silverchair.com/xsd/1/22/SCJATS-journalpublishing.xsd}permissions')\n",
    "        new_license_tag = etree.SubElement(permissions_tag, 'license')\n",
    "        new_license_tag.attrib['license-type'] = 'free'                        \n",
    "\n",
    "        logging.info('File: {}. Updating file with free license.'.format(article_path))\n",
    "\n",
    "        with open(article_path, 'wb') as doc:\n",
    "            doc.write(etree.tostring(tree, pretty_print = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_issue_xml(path_to_issue_file, df_filter, unique_headings, issue_folder_name):\n",
    "\n",
    "    folder = os.path.dirname(os.path.dirname(path_to_issue_file)).split('/')[1]\n",
    "    assets_folder = '{}/{}'.format(os.path.dirname(os.path.dirname(path_to_issue_file)), 'assets')\n",
    "    issue_num = folder.split('-')[1]\n",
    "    \n",
    "    tree = etree.parse(path_to_issue_file, parser=parser)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Some issues are missing their covers and we need to download them from the site\n",
    "    # This checks if a cover exists and if it doesn't it grabs it\n",
    "    def get_cover():\n",
    "        if glob('{}/*cover.*'.format(assets_folder)):\n",
    "            print('cover found in {}'.format(path_to_issue_file))\n",
    "            pass\n",
    "        else:\n",
    "            print('cover missing')\n",
    "            # Check if webdriver running\n",
    "            try:\n",
    "                driver.title\n",
    "                print(True)\n",
    "            except NameError as e:\n",
    "                print(False)\n",
    "                driver = webdriver.Chrome(\"/Users/kmcdouga/Dropbox (MIT)/Silverchair/Batch_2/venv/bin/chromedriver\")\n",
    "\n",
    "            # The URL to the issue homepage follows a pattern that we can stitch together using the volume and issue\n",
    "            volume = root.find('.//{http://specifications.silverchair.com/xsd/1/21/SCJATS-journalissue.xsd}volume').text\n",
    "            issue = root.find('.//{http://specifications.silverchair.com/xsd/1/21/SCJATS-journalissue.xsd}issue').text\n",
    "            toc_path = 'https://www.mitpressjournals.org/toc/{}/{}/{}'.format(JRN_CODE, volume, issue)\n",
    "            \n",
    "            try:\n",
    "                driver.get(toc_path)\n",
    "                content = driver.page_source\n",
    "                soup = BeautifulSoup(content, features='lxml')\n",
    "            \n",
    "                cover_tag = soup.find_all('img', {'alt':'Publication Cover'})\n",
    "                cover_path = 'https://www.mitpressjournals.org/{}'.format(cover_tag[0]['src'])\n",
    "                cover_name = cover_path.split('/')[-1]\n",
    "            \n",
    "                supplementary_material = etree.SubElement(root, 'supplementary-material')\n",
    "                supplementary_material.attrib['{%s}href' % nsmap['xlink']] = cover_name\n",
    "                supplementary_material.attrib['content-type'] = 'cover'\n",
    "            \n",
    "                response = requests.get(cover_path)\n",
    "                save_path = '{}/{}'.format(assets_folder, cover_name)\n",
    "                file = open(save_path, \"wb\")\n",
    "                file.write(response.content)\n",
    "                file.close()\n",
    "                driver.quit()\n",
    "            except Exception as e:\n",
    "                print('error getting cover: {}'.format(e))\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    get_cover()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Now we update the issue_meta file adding the <toc> block\n",
    "    toc_block = root.find('.//{http://specifications.silverchair.com/xsd/1/21/SCJATS-journalissue.xsd}table-of-contents')\n",
    "    toc_block.getparent().remove(toc_block)\n",
    "\n",
    "\n",
    "    table_of_contents = etree.SubElement(root, 'table-of-contents')\n",
    "    \n",
    "    # Look for articles without a heading in the TOC\n",
    "    no_heading = df_filter[pd.isna(df_filter['article_title'])]\n",
    "\n",
    "    \n",
    "#     toc_sec = etree.SubElement(table_of_contents, 'toc-sec')\n",
    "#     toc_title = etree.SubElement(toc_sec, 'title')\n",
    "\n",
    "    if not no_heading.empty:\n",
    "        toc_sec = etree.SubElement(table_of_contents, 'toc-sec')\n",
    "        toc_title = etree.SubElement(toc_sec, 'title')\n",
    "        for index, row in no_heading.iterrows():\n",
    "            toc_content = etree.SubElement(toc_sec, 'toc-content')\n",
    "            toc_content.attrib['pub-id-type'] = 'doi'\n",
    "            toc_content.attrib['{%s}href' % nsmap['xlink']] = row['article_doi']\n",
    "\n",
    "    for heading in unique_headings:\n",
    "        toc_sec = etree.SubElement(table_of_contents, 'toc-sec')\n",
    "        toc_title = etree.SubElement(toc_sec, 'title')\n",
    "        toc_title.text = heading\n",
    "        df_sec = df_filter[df_filter.article_title == heading]\n",
    "        for index, row in df_sec.iterrows():\n",
    "            toc_content = etree.SubElement(toc_sec, 'toc-content')\n",
    "            toc_content.attrib['pub-id-type'] = 'doi'\n",
    "            toc_content.attrib['{%s}href' % nsmap['xlink']] = row['article_doi']\n",
    "    return root\n",
    "\n",
    "def get_doi_list(path):\n",
    "    delete_command = 'find \\\\{} -name \\'*.DS_Store*\\' -delete'.format(path)\n",
    "    os.system(delete_command)\n",
    "\n",
    "    doi_list = ['10.1162/{}'.format(os.path.splitext(item)[0].upper()) for item in os.listdir(path)]\n",
    "    \n",
    "    df = pd.read_csv('{}.csv'.format(JRN_CODE))\n",
    "\n",
    "    def popTime(row):\n",
    "        x = row['article_doi']\n",
    "        return(x.upper())\n",
    "\n",
    "    df['article_doi'] = df.apply(popTime, axis=1)\n",
    "    df_filter = df[df.article_doi.isin(doi_list)]\n",
    "\n",
    "    unique_headings = pd.unique(df_filter['article_title'])\n",
    "    unique_headings = [value for value in unique_headings if pd.notna(value)]\n",
    "\n",
    "    return df_filter, unique_headings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Caused by line breaks in source XML. Need to find *anything* after surname and string-name and remove accordingly. \n",
    "- all files need to be formatted and indented. \n",
    "\n",
    "Need to join and normalize to start:\n",
    "F: \\s+\n",
    "R: ''\n",
    "\n",
    "From there we need to fix formatting:\n",
    "F: </surname> </\n",
    "R: </surname></\n",
    "\n",
    "F: </string-name> </\n",
    "R: </string-name></\n",
    "\n",
    "F: </given-names> </string-name>\n",
    "R: </given-names></string-name>\n",
    "\n",
    "F: </collab> </person-group>\n",
    "R: </collab></person-group> -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
